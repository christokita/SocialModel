size = 0.4) +
geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD),
width = 0,
size = 0.4) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab(expression(paste("Division of labor (", italic(D[indiv]), ")"))) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy
ggsave(gg_entropy, file = "output/SpecializationPlots/Sigma0-Epsilon0.1-BetaSweep_SDbars.png",
height = 45, width = 45, units = "mm", dpi = 800)
################################################################################
#
# Comparing various specialization plots
#
################################################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
############### Sweep across beta values ###############
####################
# Load data
####################
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.01.Rdata")
compiled_data$Model <- "Social_Beta1.01"
entropy_data <- compiled_data
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.05.Rdata")
compiled_data$Model <- "Social_Beta1.05"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1.Rdata")
compiled_data$Model <- "Social_Beta1.1"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.2.Rdata")
compiled_data$Model <- "Social_Beta1.2"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0.05-Epsilon0-Beta1.1.Rdata")
compiled_data$Model <- "Fixed"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0.05-Epsilon0-Beta1.1-Delta0.6-CHECK.Rdata")
compiled_data$Model <- "Fixed_Check"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
####################
# Summarise data
####################
# Calculate mean and SE
entropy <- entropy_data %>%
group_by(Model, n) %>%
summarise(Mean = mean(Dind),
SD = sd(Dind),
SE = sd(Dind) / sqrt(length(Dind)))
####################
# Plot
####################
pal <- c("black", "grey", brewer.pal(5, "Greens")[2:5])
gg_entropy <- ggplot(data = entropy_data, aes(x = n, colour = Model)) +
geom_point(aes(y = Dind),
size = 0.8,
alpha = 0.5,
stroke = 0) +
theme_classic() +
ylab("Division of labor") +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_colour_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1) +
facet_wrap(~Model)
gg_entropy
gg_entropy <- ggplot(data = entropy, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
ylab("Division of labor") +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_colour_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy
look <- entropy %>%
filter(Model == "Fixed_Check") %>%
mutate(n_log = log10(n)) %>%
filter(n %in% c(5, seq(10, 100, 10)))
gg_entropy_check <- ggplot(data = look, aes(x = n)) +
# geom_line(aes(y = Mean),
# size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
ylab("Division of labor") +
scale_y_continuous(limits = c(0, 1)) +
# scale_x_continuous(breaks = seq(0, 100, 20)) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy_check
################################################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
p <- 1 #prob of interact
run <- "Sigma0-Epsilon0.1-Beta1.1"
####################
# Load and process data
####################
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", run, "/"), full.names = TRUE)
soc_networks <- list()
for (i in 1:length(files)) {
load(files[i])
soc_networks[[i]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", run, "/"), full.names = TRUE)
thresh_data <- list()
for (i in 1:length(files)) {
load(files[i])
thresh_data[[i]] <- listed_data
}
# Load activity profiles
load(paste0("output/Rdata/_ProcessedData/TaskDist/", run, ".Rdata"))
task_dist <- compiled_data
task_dist$replicate <- task_dist$sim * task_dist$chunk
rm(compiled_data)
##########################################################
# Total interactions vs. activity
##########################################################
# Create space for data collection
task_activ <- task_dist
task_activ$degree <- NA
# Get data from social networks
for(i in 1:length(soc_networks)) {
# Get group size graphs
graphs <- soc_networks[[i]]
# Get individual graphs
for(j in 1:length(graphs)) {
this_graph <- graphs[[j]]
degrees <- rowSums(this_graph)
# add to dataframe
n <- 5 * i
task_activ$degree[task_activ$n == n & task_activ$replicate == j] <- degrees
}
}
# Plot
gg_act_net <- ggplot(data = task_activ, aes(x = Task1, y = degree, colour = n)) +
geom_point() +
theme_classic() +
facet_wrap(~n)
gg_act_net
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
###################
# Correlation of Average interaction partner
###################
network_correlations <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
effective_interactions <- matrix(data = rep(NA, nrow(this_graph)))
for (i in 1:nrow(this_graph)) {
effective_interactions[i] <- sum(this_graph[i,] *  thresh$ThreshBias, na.rm = TRUE) / sum(this_graph[i, ], na.rm = TRUE)
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
to_retun <- data.frame(n = nrow(this_graph), Correlation = cor(effective_interactions, thresh$ThreshBias))
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
correlation_data <- do.call('rbind', network_correlations)
correlation_data <- correlation_data %>%
group_by(Model, n) %>%
summarise(Corr_mean = mean(Correlation),
Corr_SD = sd(Correlation),
Corr_SE = sd(Correlation)/length(Correlation))
# Plot
gg_correlation <- ggplot(data = correlation_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SD, ymax = Corr_mean + Corr_SD),
width = 0,
size = 0.4) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Interaction partner correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"),
legend.background = element_blank())
gg_correlation
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
print(i*5)
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
weightcorr_data <- do.call('rbind', weighted_correlation)
weightcorr_data <- weightcorr_data %>%
group_by(Model, n) %>%
summarise(WeightedCorr_mean = mean(WeightedCorr),
WeightedCorr_SD = sd(WeightedCorr),
WeightedCorr_SE = sd(WeightedCorr)/length(WeightedCorr))
###################
# Weighted correlation (analogous to assortivity?)
###################
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
print(i*5)
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
###################
# Weighted correlation (analogous to assortivity?)
###################
library(wCorr)
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
print(i*5)
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
weightcorr_data <- do.call('rbind', weighted_correlation)
weightcorr_data <- weightcorr_data %>%
group_by(Model, n) %>%
summarise(WeightedCorr_mean = mean(WeightedCorr),
WeightedCorr_SD = sd(WeightedCorr),
WeightedCorr_SE = sd(WeightedCorr)/length(WeightedCorr))
# Plot
gg_weightedcorr <- ggplot(data = weightcorr_data, aes(x = n, y = WeightedCorr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = WeightedCorr_mean - WeightedCorr_SD, ymax = WeightedCorr_mean + WeightedCorr_SD),
width = 0,
size = 0.4) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_y_continuous(breaks = seq(-0.25, 0.05, 0.05), limits = c(-0.26, 0.05)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Assortativity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.5),
legend.key.height = unit(0.5, "line"))
gg_weightedcorr
