mod <- modularity(g_clust)
?cluster_fast_greedy
?modularity
g_clust
g <- make_full_graph(5) %du% make_full_graph(5) %du% make_full_graph(5)
g <- add_edges(g, c(1,6, 1,11, 6, 11))
fc <- cluster_fast_greedy(g)
fc
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
g_clust
this_graph <- matrix(data = c(rep(c(10, 10, 10, 10, 0, 0, 0, 0.1), 4), rep(c(0, 0, 0, 0, 10, 10, 10, 10), 4)),nrow = 8, byrow = T)
g <- graph_from_adjacency_matrix(this_graph, mode = "directed", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
p <- 1 #prob of interact
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
run = 2
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through indivi
i = 20
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency
j = 1
this_graph <- graphs[[j]]
diag(this_graph) <- 1
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
diag(this_graph) <- 0
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
p <- 1 #prob of interact
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
modularity <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- 1 # CHANGE BACK TO ZERO
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
clust_coeff <- transitivity(graph = g, type = "weighted", weights = E(g)$weight)
# return
replicate_row <- data.frame(n = nrow(this_graph),
Modularity = mod,
ClustCoeff =  mean(clust_coeff, na.rm = TRUE))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
mod_data <- do.call("rbind", modularity)
mod_data <- mod_data %>%
group_by(Model, n) %>%
summarise(Modul_mean = mean(Modularity),
Modul_SD = sd(Modularity),
Modul_SE = sd(Modularity)/length(Modularity))
# Plot
gg_mod <- ggplot(mod_data, aes(x = n, y = Modul_mean, colour = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Modul_mean - Modul_SD, ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_y_continuous(breaks = seq(0, 0.012, 0.002), limits = c(-0.0002, 0.012)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod
# Plot
gg_mod <- ggplot(mod_data, aes(x = n, y = Modul_mean, colour = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Modul_mean - Modul_SD, ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
# scale_y_continuous(breaks = seq(0, 0.012, 0.002), limits = c(-0.0002, 0.012)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod
rowSums(this_graph)
modularity <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- rowSums(this_graph) # CHANGE BACK TO ZERO
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
clust_coeff <- transitivity(graph = g, type = "weighted", weights = E(g)$weight)
# return
replicate_row <- data.frame(n = nrow(this_graph),
Modularity = mod,
ClustCoeff =  mean(clust_coeff, na.rm = TRUE))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
mod_data <- do.call("rbind", modularity)
mod_data <- mod_data %>%
group_by(Model, n) %>%
summarise(Modul_mean = mean(Modularity),
Modul_SD = sd(Modularity),
Modul_SE = sd(Modularity)/length(Modularity))
# Plot
gg_mod <- ggplot(mod_data, aes(x = n, y = Modul_mean, colour = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Modul_mean - Modul_SD, ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
# scale_y_continuous(breaks = seq(0, 0.012, 0.002), limits = c(-0.0002, 0.012)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod
modularity <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- 0
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
clust_coeff <- transitivity(graph = g, type = "weighted", weights = E(g)$weight)
# return
replicate_row <- data.frame(n = nrow(this_graph),
Modularity = mod,
ClustCoeff =  mean(clust_coeff, na.rm = TRUE))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
mod_data <- do.call("rbind", modularity)
mod_data <- mod_data %>%
group_by(Model, n) %>%
summarise(Modul_mean = mean(Modularity),
Modul_SD = sd(Modularity),
Modul_SE = sd(Modularity)/length(Modularity))
# Plot
gg_mod <- ggplot(mod_data, aes(x = n, y = Modul_mean, colour = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Modul_mean - Modul_SD, ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
# scale_y_continuous(breaks = seq(0, 0.012, 0.002), limits = c(-0.0002, 0.012)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod
# Plot
gg_mod <- ggplot(mod_data, aes(x = n, y = Modul_mean, colour = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Modul_mean - Modul_SD, ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_y_continuous(breaks = seq(0, 0.012, 0.002), limits = c(-0.0002, 0.012)) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod
g_adj
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- c(70) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 1000 #number of generations to run simulation
reps           <- 1 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 1 #baseline probablity of initiating an interaction per time step
epsilon        <- 0.1 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
####################
# Run ensemble simulation
####################
# Prep meta-lists for collection of group size simulations
groups_taskDist    <- list()
groups_stim        <- list()
groups_thresh      <- list()
groups_entropy     <- list()
groups_graphs      <- list()
# Loop through group sizes
for (i in 1:length(Ns)) {
# Set group size
n <- Ns[i]
# Prep lists for collection of simulation outputs from this group size
ens_taskDist    <- list()
ens_entropy     <- list()
ens_stim        <- list()
ens_thresh      <- list()
ens_graphs      <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
# Create tasktally table
stimMat <- cbind(stimMat, 0:(nrow(stimMat) - 1))
colnames(stimMat)[ncol(stimMat)] <- "t"
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_stim[[sim]]        <- stimMat
ens_thresh[[sim]]      <- threshMat
ens_graphs[[sim]]      <- g_tot / gens
}
# Add to list of lists
groups_taskDist[[i]]    <- ens_taskDist
groups_stim[[i]]        <- ens_stim
groups_thresh[[i]]      <- ens_thresh
groups_entropy[[i]]     <- ens_entropy
groups_graphs[[i]]      <- ens_graphs
}
g_adj
rowSums(g_adj)
colSums(g_adj)
g_adj <- matrix(data = rep(0, dimension*dimension), ncol = dimension)
X_sub_g <- X_g
X_g
colSums(X_g)
70*0.4
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
i =1
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
X-g
X_g
i = 2
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
task
# set up list of potential connections
potential <- seq(1:dimension)
dimension <- nrow(X_sub_g)
g_adj <- matrix(data = rep(0, dimension*dimension), ncol = dimension)
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
potential
baseline_prob
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
baseline_prob[same] <- baseline_prob[same] * bias
bias = 1.1
baseline_prob[same] <- baseline_prob[same] * bias
baseline_prob
potential <- potential[-i] # remove self
potential
baseline_prob <- baseline_prob[-i] # remove self
baseline_prob
connection <- sample(x = potential, size = 1, prob = baseline_prob)
sample(x = potential, size = 10, prob = baseline_prob, replace = T)
test <- sample(x = potential, size = 100, prob = baseline_prob, replace = T)
test
X_g[test]
X_g[test,]
colSums(X_g[test,])
test <- sample(x = potential, size = 1000, prob = baseline_prob, replace = T)
colSums(X_g[test,])
437/358
437-358
79*0.1
7.9*20
