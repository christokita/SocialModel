interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals - 1)
interact_bias <- thresh$ThreshBias[ind]
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
interact_bias <- thresh$ThreshBias
###################
# Weighted correlation (analogous to assortivity?)
###################
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals - 1)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
ind = 1
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals - 1)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
interact_weight
length(interact_weight)
###################
# Weighted correlation (analogous to assortivity?)
###################
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
###################
# Weighted correlation (analogous to assortivity?)
###################
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
print(i)
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
##########################################################
# Assortivity
##########################################################
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
###################
# Correlation of Average interaction partner
###################
network_correlations <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- matrix(data = rep(NA, length(this_graph)), ncol = ncol(this_graph))
for (i in 1:nrow(this_graph)) {
# social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
effective_interactions <- rowSums(social_interaction, na.rm = T)
to_retun <- data.frame(n = nrow(this_graph), Correlation = cor(effective_interactions, thresh$ThreshBias))
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
correlation_data <- do.call('rbind', network_correlations)
correlation_data <- correlation_data %>%
group_by(Model, n) %>%
summarise(Corr_mean = mean(Correlation),
Corr_SE = sd(Correlation)/length(Correlation))
# Plot
gg_correlation <- ggplot(data = correlation_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SE, ymax = Corr_mean + Corr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Social network correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"))
gg_correlation
# Plot
gg_correlation <- ggplot(data = correlation_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SE, ymax = Corr_mean + Corr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Social network correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"))
gg_correlation
ggsave(gg_correlation, filename = "Output/Networks/NetworkMetrics/CorrelationInNetwork.png",
height = 45, width = 45, units = "mm", dpi = 400)
ggsave(gg_correlation, filename = "Output/Networks/NetworkMetrics/CorrelationInNetwork.svg",
height = 45, width = 45, units = "mm")
###################
# Weighted correlation (analogous to assortivity?)
###################
weighted_correlation <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
print(i)
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
number_individuals <- dim(this_graph)[1]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- data.frame(ThreshBias = NULL, InteractBias = NULL, InteractWeight = NULL)
for (ind in 1:nrow(this_graph)) {
# Calculate bias and weighted interaction
thresh_bias <- rep(thresh$ThreshBias[ind], number_individuals)
interact_bias <- thresh$ThreshBias
interact_weight <- this_graph[ind, ]
interact_weight <- interact_weight[!is.na(interact_weight)]
# Bind to dataframe
to_bind <- data.frame(ThreshBias = thresh_bias, InteractBias = interact_bias, InteractWeight = interact_weight)
social_interaction <- rbind(social_interaction, to_bind)
}
# Filter and return
weighted_corr <- weightedCorr(x = social_interaction$ThreshBias,
y = social_interaction$InteractBias,
method = "pearson",
weights = social_interaction$InteractWeight)
to_return <- data.frame(n = number_individuals, WeightedCorr = weighted_corr)
# return
return(to_return)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
weightcorr_data <- do.call('rbind', weighted_correlation)
weightcorr_data <- weightcorr_data %>%
group_by(Model, n) %>%
summarise(WeightedCorr_mean = mean(WeightedCorr),
WeightedCorr_SE = sd(WeightedCorr)/length(WeightedCorr))
# Plot
gg_weightedcorr <- ggplot(data = weightcorr_data, aes(x = n, y = WeightedCorr_mean,
colour = Model, group = Model, fill = Model)) +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = WeightedCorr_mean - WeightedCorr_SE, ymax = WeightedCorr_mean + WeightedCorr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Weighted Correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.5),
legend.key.height = unit(0.5, "line"))
gg_weightedcorr
# Plot
gg_weightedcorr <- ggplot(data = weightcorr_data, aes(x = n, y = WeightedCorr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "blue") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = WeightedCorr_mean - WeightedCorr_SE, ymax = WeightedCorr_mean + WeightedCorr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
scale_linetype_manual(name = "Threshold",
values = c("dotted", "solid")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Weighted Correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.5),
legend.key.height = unit(0.5, "line"))
gg_weightedcorr
round(10.1)
round(10.1, digits = 2)
round(10.00, digits = 2)
ceiling(10.1)
thresh$ThreshBias
ceiling(thresh$ThreshBias)
mixmat <- function(mygraph, attrib, use.density=TRUE) {
require(igraph)
# get unique list of characteristics of the attribute
attlist <- sort(unique(get.vertex.attribute(mygraph,attrib)))
numatts <- length(attlist)
# build an empty mixing matrix by attribute
mm <- matrix(nrow=numatts,
ncol=numatts,
dimnames=list(attlist,attlist))
# calculate edge density for each matrix entry by pairing type
# lends itself to parallel if available
el <- get.edgelist(mygraph,names=FALSE)
for (i in 1:numatts) {
for (j in 1:numatts) {
mm[i,j] <- length(which(apply(el,1,function(x) {
get.vertex.attribute(mygraph, attrib, x[1] ) == attlist[i] &&
get.vertex.attribute(mygraph, attrib, x[2] ) == attlist[j]  } )))
}
}
# convert to proportional mixing matrix if desired (ie by edge density)
if (use.density) mm/ecount(mygraph) else mm
}
# Calculate using iGraph
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T, diag = F)
V(g)$Bias <- thresh$ThreshBias
g
mixmat(mygraph = g, attrib = V(g)$Bias)
mixmat(mygraph = g, attrib = "Bias")
# Calculate using iGraph
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T, diag = F)
V(g)$Bias <- thresh$ThreshBias
V(g)$Bias
mixmat(mygraph = g, attrib = "Bias")
get.vertex.attribute(g,"bias")
get.vertex.attribute(g,"Bias")
get.vertex.attribute(g,V(g)$Bias)
?get.vertex.attribute
library(igraph)
mixmat(mygraph = g, attrib = "Bias")
detach(statnet)
detach("statnet")
detach(package:statnet
)
library(igraph)
mixmat(mygraph = g, attrib = "Bias")
?get.vertex.attribute
