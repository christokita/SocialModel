####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
# Bind together and summarise
entropy_sum <- do.call("rbind", ens_entropy)
View(entropy_sum)
k = 2
# Set group size
n <- run_in_parallel[k, 1]
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
betas          <- seq(1, 1, 0.005) #probability of interacting with individual in same state relative to others
betas
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
betas          <- seq(1, 1.01, 0.005) #probability of interacting with individual in same state relative to others
n = 4
k = 1
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, beta = betas)
run_in_parallel <- run_in_parallel %>%
arrange(n)
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
X_g
X_sub_g = X_g
prob_interact = p
bias = beta
dimension <- nrow(X_sub_g)
g_adj <- matrix(data = rep(0, dimension*dimension), ncol = dimension)
g_adj
# Determine if interact
interact <- sample(x = c(1, 0), size = 1, prob = c(prob_interact, 1 - prob_interact))
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
i = 1
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
# catch for if there is only two individuals
if (length(baseline_prob) == 1) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
potential
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
connection <- sample(x = potential, size = 1, prob = baseline_prob)
potential
baseline_prob
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
baseline_prob
# catch for if there is only two individuals
if (length(baseline_prob) == 1) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
baseline_prob
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
task
X_g
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
i
colNanmes(same)
names(same)
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[!i , task] == 1)
same
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
connection <- sample(x = potential, size = 1, prob = baseline_prob)
baseline_prob
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# catch for if there is only two individuals
if (length(baseline_prob) == 2) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
baseline_prob[same] <- baseline_prob[same] * bias
baseline_prob
baseline_prob[same] <- baseline_prob[same] * bias
bias
bias = 2
baseline_prob[same] <- baseline_prob[same] * bias
baseline_prob
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
potential
baseline_prob
connection <- sample(x = potential, size = 1, prob = baseline_prob)
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# catch for if there is only two individuals
if (length(baseline_prob) == 2) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
connection <- sample(x = potential, size = 1, prob = baseline_prob)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(2, 100, 2) #vector of number of individuals to simulate
betas          <- seq(1, 1.25, 0.005) #probability of interacting with individual in same state relative to others
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, beta = betas)
run_in_parallel <- run_in_parallel %>%
arrange(n)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
epsilons       <- seq(0, 0.5, 0.025) #relative weighting of social interactions for adjusting thresholds
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, epsilon = epsilons)
run_in_parallel <- run_in_parallel %>%
arrange(n)
epsilons       <- seq(0, 0.5, 0.01) #relative weighting of social interactions for adjusting thresholds
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 2) #vector of number of individuals to simulate
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, epsilon = epsilons)
run_in_parallel <- run_in_parallel %>%
arrange(n)
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(2, 100, 2) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 0.5 #baseline probablity of initiating an interaction per time step
epsilons       <- seq(0, 0.5, 0.01) #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, epsilon = epsilons)
run_in_parallel <- run_in_parallel %>%
arrange(n)
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(2, 100, 2) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 0.5 #baseline probablity of initiating an interaction per time step
epsilon        <- 0.1 #relative weighting of social interactions for adjusting thresholds
betas          <- seq(1, 1.25, 0.005) #probability of interacting with individual in same state relative to others
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, beta = betas)
run_in_parallel <- run_in_parallel %>%
arrange(n)
##########################################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
directory_path <- "output/Rdata/Sigma0-Epsilon0.1-Beta1.1/"
output_path <- "output/Rdata/_ProcessedData/"
run_info <- gsub("^.*(Sigma.*)/$", "\\1", directory_path, perl = TRUE)
####################
# Create folders
####################
# Get names of folders
folders <- list.files(directory_path)
output_folders <- list.files(output_path)
# Create folders for those not existing in processed data
missing_folders <- folders[!folders %in% output_folders]
for (missing_folder in missing_folders) {
dir.create(paste0(output_path, missing_folder))
}
# Divide up folders into those where bound dataframes will be made
# and those where compiled lists will be made (for the sake of memory)
bind_folders <- folders[folders %in% c("Entropy", "TaskDist")]
list_folders <- folders[folders %in% c("Graphs", "Thresh", "Stim", "TaskTally", "Thresh1Time", "Thresh2Time")]
####################
####################
# Load, compile (lists), and save data
####################
for (folder in list_folders) {
# Create subfolder to story data by group size
full_output_path <- paste0(output_path, folder, "/", run_info, "/")
dir.create(full_output_path, showWarnings = FALSE)
# Get files
files <- list.files(paste0(directory_path, folder), full.names = TRUE)
# Get group sizes
group_sizes <- gsub(".*/([0-9]+)-[0-9]+.Rdata", "\\1", files, perl = TRUE)
group_sizes <- unique(group_sizes)
# Loop through group sizes
# Graphs are stored with different name
if (folder == "Graphs") {
for (i in group_sizes) {
group_files <- files[grepl(paste0(".*/", i, "-[0-9]+.Rdata"), files)]
# Loop through the files for this group size and bind
for (file in group_files) {
# Load data
load(file)
# Bind together
ifelse(!exists("listed_data"),  listed_data <- ens_graphs, listed_data <- c(listed_data, ens_graphs))
}
save(listed_data, file = paste0(full_output_path, i, ".Rdata"))
rm(listed_data)
}
# Check if threshold time series (massive so just move, don't process)
} else if (folder %in% c("Thresh1Time", "Thresh2Time")) {
file.copy(from = files, to = full_output_path)
# Otherwise bind together normally
} else {
for (i in group_sizes) {
group_files <- files[grepl(paste0(".*/", i, "-[0-9]+.Rdata"), files)]
# Loop through the files for this group size and bind
for (file in group_files) {
# Load data
load(file)
# Bind together
ifelse(!exists("listed_data"),  listed_data <- data, listed_data <- rbind(listed_data, data))
}
save(listed_data, file = paste0(full_output_path, i, ".Rdata"))
rm(listed_data)
}
}
}
