scale_y_continuous(limits = c(0, 1)) +
scale_color_manual(name = "Threshold type",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold type",
values = c("#ffffff", "#4d4d4d")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("% Non-random interactions") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = "none")
gg_interactions
source('~/Documents/Research/Tarnita Lab/Mixing Experiments/MixingModel/scripts/1_MixingTest.R', echo=TRUE)
################################################################################
#
# Social interaction model: set for running on Della cluster
#
################################################################################
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
chunk_size     <- 5 #number of simulations sent to single core
# Threshold Parameters
ThreshM        <- rep(0, m) #population threshold means
ThreshSD       <- ThreshM * 0.05 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 1 #baseline probablity of initiating an interaction per time step
epsilon        <- 0 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
################################################################################
#
# Social interaction model: set for running on Della cluster
#
################################################################################
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
chunk_size     <- 5 #number of simulations sent to single core
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0.05 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 1 #baseline probablity of initiating an interaction per time step
epsilon        <- 0 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
storage_path <- "/scratch/gpfs/ctokita/"
dir_name <- paste0("Sigma", ThreshSD[1], "-Epsilon", epsilon, "-Beta", beta)
dir_name
dir_name <- paste0("Sigma",  (ThreshSD/ThreshM)[1], "-Epsilon", epsilon, "-Beta", beta)
dir_name
################################################################################
#
# Analyzing social network features comapred to activity and other factors
#
################################################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
p <- 1 #prob of interact
run <- "Sigma0-Epsilon0.1-Beta1.1"
####################
# Load and process data
####################
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", run, "/"), full.names = TRUE)
soc_networks <- list()
for (i in 1:length(files)) {
load(files[i])
soc_networks[[i]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", run, "/"), full.names = TRUE)
thresh_data <- list()
for (i in 1:length(files)) {
load(files[i])
thresh_data[[i]] <- listed_data
}
# Load activity profiles
load(paste0("output/Rdata/_ProcessedData/TaskDist/", run, ".Rdata"))
task_dist <- compiled_data
task_dist$replicate <- task_dist$sim * task_dist$chunk
rm(compiled_data)
##########################################################
# Total interactions vs. activity
##########################################################
# Create space for data collection
task_activ <- task_dist
task_activ$degree <- NA
# Get data from social networks
for(i in 1:length(soc_networks)) {
# Get group size graphs
graphs <- soc_networks[[i]]
# Get individual graphs
for(j in 1:length(graphs)) {
this_graph <- graphs[[j]]
degrees <- rowSums(this_graph)
# add to dataframe
n <- 5 * i
task_activ$degree[task_activ$n == n & task_activ$replicate == j] <- degrees
}
}
# Plot
gg_act_net <- ggplot(data = task_activ, aes(x = Task1, y = degree, colour = n)) +
geom_point() +
theme_classic() +
facet_wrap(~n)
gg_act_net
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
###################
###################
# Correlation of Average interaction partner
###################
network_correlations <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- matrix(data = rep(NA, length(this_graph)), ncol = ncol(this_graph))
for (i in 1:nrow(this_graph)) {
# social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
effective_interactions <- rowSums(social_interaction, na.rm = T)
to_retun <- data.frame(n = nrow(this_graph), Correlation = cor(effective_interactions, thresh$ThreshBias))
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
correlation_data <- do.call('rbind', network_correlations)
correlation_data <- correlation_data %>%
group_by(Model, n) %>%
summarise(Corr_mean = mean(Correlation),
Corr_SE = sd(Correlation)/length(Correlation))
# Plot
gg_correlation <- ggplot(data = correlation_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SE, ymax = Corr_mean + Corr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Interaction partner correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"),
legend.background = element_blank())
gg_correlation
i = 2
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
j = 1
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- matrix(data = rep(NA, length(this_graph)), ncol = ncol(this_graph))
this_graph
for (i in 1:nrow(this_graph)) {
# social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
View(social_interaction)
social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
for (i in 1:nrow(this_graph)) {
social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
View(social_interaction)
i
this_graph
file
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
run
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
run
run = runs[2]
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
run = 2
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
i = 7
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
j = 1
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
social_interaction <- matrix(data = rep(NA, length(this_graph)), ncol = ncol(this_graph))
social_interaction
this_graph
this_graph[1,]
thresh$ThreshBias
for (i in 1:nrow(this_graph)) {
social_interaction[i, ] <- this_graph[i,] *  thresh$ThreshBias
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
i = 1
this_graph[i,] *  thresh$ThreshBias
sum(this_graph[i,] *  thresh$ThreshBias)
sum(this_graph[i,] *  thresh$ThreshBias, na.rm = T)
sum(this_graph[i,] *  thresh$ThreshBias, na.rm = T) / sum(this_graph[i, ], na.rm = T)
# Multiply to get bias weighted by interaction frequenchy
effective_interactions <- matrix(data = rep(NA, nrow(this_graph)))
effective_interactions
effective_interactions <- matrix(data = rep(NA, nrow(this_graph)))
for (i in 1:nrow(this_graph)) {
effective_interactions[i] <- sum(this_graph[i,] *  thresh$ThreshBias, na.rm = TRUE)
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
###################
# Correlation of Average interaction partner
###################
network_correlations <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Multiply to get bias weighted by interaction frequenchy
effective_interactions <- matrix(data = rep(NA, nrow(this_graph)))
for (i in 1:nrow(this_graph)) {
effective_interactions[i] <- sum(this_graph[i,] *  thresh$ThreshBias, na.rm = TRUE) / sum(this_graph[i, ], na.rm = TRUE)
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
to_retun <- data.frame(n = nrow(this_graph), Correlation = cor(effective_interactions, thresh$ThreshBias))
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
correlation_data <- do.call('rbind', network_correlations)
correlation_data <- correlation_data %>%
group_by(Model, n) %>%
summarise(Corr_mean = mean(Correlation),
Corr_SE = sd(Correlation)/length(Correlation))
# Plot
gg_correlation <- ggplot(data = correlation_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SE, ymax = Corr_mean + Corr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Interaction partner correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"),
legend.background = element_blank())
gg_correlation
thresh$ThreshBias
mean(thresh$ThreshBias)
abs(thresh$ThreshBias - mean(thresh$ThreshBias))
gg_correlation
ggsave(gg_correlation, filename = "Output/Networks/NetworkMetrics/CorrelationInNetwork.png",
height = 45, width = 45, units = "mm", dpi = 400)
ggsave(gg_correlation, filename = "Output/Networks/NetworkMetrics/CorrelationInNetwork.svg",
height = 45.5, width = 45.5, units = "mm")
i
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
i = 7
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
j = 1
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Calculate mean bias in network
mean_bias <- mean(thresh$ThreshBias)
thresh$BiasDeviation <- thresh$ThreshBias - mean_bias
View(thresh)
###################
# Local deviation from mean theshold bias
###################
local_bias <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- NA
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Calculate mean bias in network and individual deviation from this
mean_bias <- mean(thresh$ThreshBias)
thresh$BiasDeviation <- thresh$ThreshBias - mean_bias
# Multiply to get bias weighted by interaction frequenchy
effective_interactions <- matrix(data = rep(NA, nrow(this_graph)))
for (i in 1:nrow(this_graph)) {
effective_interactions[i] <- sum(this_graph[i,] *  thresh$BiasDeviation, na.rm = TRUE) / sum(this_graph[i, ], na.rm = TRUE)
# social_interaction[i, ] <- (this_graph[i,] / sum(this_graph[i,], na.rm = T)) *  thresh$ThreshBias
}
to_retun <- data.frame(n = nrow(this_graph), Correlation = cor(effective_interactions, thresh$BiasDeviation))
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$Model <- run_names[run]
return(interaction_info)
})
# Bind
local_bias_data <- do.call('rbind', local_bias)
local_bias_data <- local_bias_data %>%
group_by(Model, n) %>%
summarise(Corr_mean = mean(Correlation),
Corr_SE = sd(Correlation)/length(Correlation))
# Plot
gg_localbias <- ggplot(data = local_bias_data, aes(x = n, y = Corr_mean,
colour = Model, group = Model, fill = Model)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
geom_line(size = 0.4) +
geom_errorbar(aes(ymin = Corr_mean - Corr_SE, ymax = Corr_mean + Corr_SE),
width = 0) +
geom_point(size = 0.8, shape = 21) +
scale_color_manual(name = "Threshold",
values = c("#878787", "#4d4d4d")) +
scale_fill_manual(name = "Threshold",
values = c("#ffffff", "#4d4d4d")) +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab("Interaction partner correlation") +
theme_ctokita() +
theme(aspect.ratio = 1,
legend.position = c(0.8, 0.7),
legend.key.height = unit(0.5, "line"),
legend.background = element_blank())
gg_localbias
