P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
# threshold_max = 2 * ThreshM[1])
threshold_max = 100)
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- label_parallel_runs(matrix = entropy, n = n, simulation = sim, chunk = chunk)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
totalTaskDist <- label_parallel_runs(matrix = totalTaskDist, n = n, simulation = sim, chunk = chunk)
# Create thresh table
threshMat <- label_parallel_runs(matrix = threshMat, n = n, simulation = sim, chunk = chunk)
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_thresh[[sim]]      <- threshMat
ens_graphs[[sim]]      <- g_tot / gens
}
View(X_g)
View(X_tot)
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
library(viridis)
library(ggridges)
####################
# Load and process data
####################
# Load entropy
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1-P0.5.Rdata")
entropy_data <- compiled_data
entropy_data$Model <- "Ep0.1-P0.5"
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1.Rdata")
compiled_data$Model <- "Ep0.1-P1"
entropy_data <- rbind(entropy_data, compiled_data)
# Summarise
entropy_data <- entropy_data %>%
group_by(Model, n) %>%
summarise(Mean = mean(Dind),
SE = sd(Dind) / sqrt(length(Dind)))
####################
# Plot
####################
pal <- c("#a6cee3", "#1f78b4", "#33a02c")
gg_int <- ggplot(data = entropy_data, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab(expression(paste("Division of labor (", italic(D[indiv]), ")"))) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c(expression(paste(epsilon, " = 0.1, ", p, " = 0.5")),
expression(paste(epsilon, " = 0.1, ", p, " = 1"))),
name = "Model") +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "right",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
legend.text.align = 0,
aspect.ratio = 1)
gg_int
View(entropy_data)
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
chunk_size     <- 5 #number of simulations sent to single core
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
n = 100
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
threshMat
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
chunk_size     <- 5 #number of simulations sent to single core
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 0.5 #baseline probablity of initiating an interaction per time step
epsilon        <- 0.1 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
n = 40
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
# threshold_max = 2 * ThreshM[1])
threshold_max = 100)
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- label_parallel_runs(matrix = entropy, n = n, simulation = sim, chunk = chunk)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
totalTaskDist <- label_parallel_runs(matrix = totalTaskDist, n = n, simulation = sim, chunk = chunk)
# Create thresh table
threshMat <- label_parallel_runs(matrix = threshMat, n = n, simulation = sim, chunk = chunk)
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_thresh[[sim]]      <- threshMat
ens_graphs[[sim]]      <- g_tot / gens
entropy
p
plot(threshMat)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/2b_Process_ClusterSimData.R', echo=TRUE)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1-P0.5.Rdata")
entropy_data <- compiled_data
entropy_data$Model <- "Ep0.1-P0.5"
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1.Rdata")
compiled_data$Model <- "Ep0.1-P1"
entropy_data <- rbind(entropy_data, compiled_data)
# Summarise
entropy_data <- entropy_data %>%
group_by(Model, n) %>%
summarise(Mean = mean(Dind),
SE = sd(Dind) / sqrt(length(Dind)))
####################
# Plot
####################
pal <- c("#a6cee3", "#1f78b4", "#33a02c")
gg_int <- ggplot(data = entropy_data, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab(expression(paste("Division of labor (", italic(D[indiv]), ")"))) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c(expression(paste(epsilon, " = 0.1, ", p, " = 0.5")),
expression(paste(epsilon, " = 0.1, ", p, " = 1"))),
name = "Model") +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "right",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
legend.text.align = 0,
aspect.ratio = 1)
gg_int
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/supp_analysis/Analyze_InteractProb.R', echo=TRUE)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/2b_Process_ClusterSimData.R', echo=TRUE)
################################################################################
#
# Analyzing social network features comapred to activity and other factors
#
################################################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
p <- 1 #prob of interact
run <- "Sigma0-Epsilon0.1-Beta1.1"
####################
# Load and process data
####################
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", run, "/"), full.names = TRUE)
soc_networks <- list()
for (i in 1:length(files)) {
load(files[i])
soc_networks[[i]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", run, "/"), full.names = TRUE)
thresh_data <- list()
for (i in 1:length(files)) {
load(files[i])
thresh_data[[i]] <- listed_data
}
# Load activity profiles
load(paste0("output/Rdata/_ProcessedData/TaskDist/", run, ".Rdata"))
task_dist <- compiled_data
task_dist$replicate <- task_dist$sim * task_dist$chunk
rm(compiled_data)
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
p <- 1 #prob of interact
runs <- c("Sigma0.05-Epsilon0-Beta1.1",
"Sigma0-Epsilon0.1-Beta1.1")
run_names <- c("Fixed", "Social")
run =1
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
i = 8
graphs <- soc_networks[[i]]
replicates <- length(graphs)
j = 1
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- NA
this_graph
g <- graph_from_adjacency_matrix(this_graph, weighted = T)
transitivity(g)
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
transitivity(g)
diag(this_graph) <- 0
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
transitivity(g)
i
i = 1
graphs <- soc_networks[[i]]
replicates <- length(graphs)
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
transitivity(g)
cluster_walktrap(g)
i = 8
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
cluster_walktrap(g)
cluster_walktrap(g, weights = E(g)$Weight)
i = 20
runs
run = 2
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop
i
graphs <- soc_networks[[i]]
replicates <- length(graphs)
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
transitivity(g)
cluster_walktrap(g, weights = E(g)$Weight)
i
i = 2
graphs <- soc_networks[[i]]
replicates <- length(graphs)
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
transitivity(g)
cluster_walktrap(g, weights = E(g)$Weight)
cluster_walktrap(g, weights = E(g)$weight)
cluster_walktrap(g, weights = E(g)$Weight)
g_clust <- cluster_walktrap(g, weights = E(g)$Weight)
modularity(g_clust)
modularity(g, membership(g_clust))
nrow(this_graph)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
g_clust <- cluster_walktrap(g, weights = E(g)$Weight)
# return
replicate_row <- data.frame(Model = run_names[run],
n = nrow(this_graph),
Modularity = modularity(g_clust),
ClustCoeff =  transitivity(g))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
View(size_data)
interaction_rates <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
g_clust <- cluster_walktrap(g, weights = E(g)$Weight)
# return
replicate_row <- data.frame(Model = run_names[run],
n = nrow(this_graph),
Modularity = modularity(g_clust),
ClustCoeff =  transitivity(g))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
return(interaction_info)
})
clustering <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- NA
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = T)
g_clust <- cluster_walktrap(g, weights = E(g)$Weight)
# return
replicate_row <- data.frame(Model = run_names[run],
n = nrow(this_graph),
Modularity = modularity(g_clust),
ClustCoeff =  transitivity(g))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
return(interaction_info)
})
d
# Bind
cluster_data <- do.call("rbind", clustering)
View(cluster_data)
# Plot
gg_clust <- ggplot(cluster_data, aes(x = n, y = Modularity, colour = Model)) +
geom_point() +
theme_ctokita()
gg_clust
