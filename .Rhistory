# Hide the legend (optional)
legend.position = "none",
legend.key.width = unit(3, "mm"),
legend.key.height = unit(6, "mm"),
legend.title = element_text(size = 7),
legend.text = element_text(size = 6),
panel.border = element_rect(size = 0.3, fill = NA, colour = "black"),
plot.title = element_blank(),
aspect.ratio = 1,
panel.grid = element_blank()) +
ggtitle(paste0("Group Size = ", groupsize))
# return graph
print(groupsize)
return(gg_avg_adj)
})
# Save single plot
gg_simp <- simple_graphs[80/5]
gg_simp
ggsave("output/Networks/RawPlots/SimpleAdjPlot_80.svg", width = 38, height = 38, units = "mm")
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3_AnalyzeParaSpace.R', echo=TRUE)
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
############### Sweep across beta values ###############
####################
# Load data
####################
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.01.Rdata")
compiled_data$Model <- "Social_Beta1.01"
entropy_data <- compiled_data
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.05.Rdata")
compiled_data$Model <- "Social_Beta1.05"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.1.Rdata")
compiled_data$Model <- "Social_Beta1.1"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
load("output/Rdata/_ProcessedData/Entropy/Sigma0-Epsilon0.1-Beta1.2.Rdata")
compiled_data$Model <- "Social_Beta1.2"
entropy_data <- rbind(entropy_data, compiled_data)
rm(compiled_data)
####################
# Summarise data
####################
# Calculate mean and SE
entropy <- entropy_data %>%
group_by(Model, n) %>%
summarise(Mean = mean(Dind),
SE = sd(Dind) / sqrt(length(Dind)))
####################
# Plot
####################
pal <- brewer.pal(5, "Greens")[2:5]
gg_entropy <- ggplot(data = entropy, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
ylab("Division of labor") +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy
gg_entropy <- ggplot(data = entropy, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab(expression(paste("Division of labor (", italic(D{indiv}), ")"))) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy <- ggplot(data = entropy, aes(x = n, colour = Model)) +
geom_line(aes(y = Mean),
size = 0.4) +
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE),
width = 0) +
geom_point(aes(y = Mean),
size = 0.8) +
theme_classic() +
xlab(expression(paste("Group Size (", italic(n), ")"))) +
ylab(expression(paste("Division of labor (", italic(D[indiv]), ")"))) +
scale_x_continuous(breaks = seq(0, 100, 20)) +
scale_color_manual(values = pal,
labels = c("1.2", "1.1"),
name = expression("Interaction bias"(Beta))) +
theme(axis.text = element_text(colour = "black", size = 6),
axis.title = element_text(size = 7, face = "italic"),
legend.position = "none",
legend.title = element_text(size = 7,
face = "bold"),
legend.text = element_text(size = 6),
legend.key.height = unit(4, "mm"),
legend.key.width = unit(5, "mm"),
axis.ticks = element_line(size = 0.3, color = "black"),
axis.line = element_line(size = 0.3, color = "black"),
aspect.ratio = 1)
gg_entropy
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/Analyze_Entropy.R', echo=TRUE)
ggsave(gg_entropy, file = "output/SpecializationPlots/Sigma0-Beta1.1-EpsSweep.svg",
height = 48, width = 48, units = "mm")
ggsave(gg_entropy, file = "output/SpecializationPlots/Sigma0-Beta1.1-EpsSweep.svg",
height = 48.5, width = 48.5, units = "mm")
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/Analyze_Entropy.R', echo=TRUE)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/Analyze_Entropy.R', echo=TRUE)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/Analyze_Entropy.R', echo=TRUE)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/Analyze_Entropy.R', echo=TRUE)
k = 1
################################################################################
#
# Social interaction model: set for running on Della cluster
#
################################################################################
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
library(parallel)
library(snowfall)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 100 #number of replications per simulation (for ensemble)
chunk_size     <- 5 #number of simulations sent to single core
corrStep       <- 200 #number of time steps for calculation of correlation
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
# Social Network Parameters
p              <- 1 #baseline probablity of initiating an interaction per time step
epsilon        <- 0 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Set group size
n <- run_in_parallel[k, 1]
chunk <- run_in_parallel[k, 2]
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Break up parameter replications into smaller batches\
chunk_run  <- 1:(reps / chunk_size)
run_in_parallel <- expand.grid(n = Ns, run = chunk_run)
run_in_parallel <- run_in_parallel %>%
arrange(n)
# Set group size
n <- run_in_parallel[k, 1]
chunk <- run_in_parallel[k, 2]
k = 1
# Set group size
n <- run_in_parallel[k, 1]
chunk <- run_in_parallel[k, 2]
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
gens           <- 100 #number of generations to run simulation
gens           <- 1000 #number of generations to run simulation
# Run Simulations
for (sim in 1:chunk_size) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
taskStep <- list()
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
# Create time step for correlation
if (t %% corrStep == 0) {
# Get tasks performance in correlation step
X_step <- X_tot - X_prevTot
# Add to ensemble list of task steps
taskStep[[t / corrStep]] <- X_step
# Calculate rank correlation if it is not the first step
if(sum(X_prev) != 0) {
# Normalize
stepNorm <- X_step / rowSums(X_step)
prevNorm <- X_prev / rowSums(X_prev)
# Calculate ranks
step_ranks <- calculateTaskRank(TaskStepMat = X_step)
prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)
# Calculate Correlation
rankCorr <- cor(prev_ranks, step_ranks, method = "spearman")
# Put in list
taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)
names(taskCorr)[(t / corrStep) - 1] <- paste0("Gen", t)
}
# Update previous step total matrix
X_prevTot <- X_tot
# Update previous step total matrix
X_prev <- X_step
}
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- label_parallel_runs(matrix = entropy, n = n, simulation = sim, chunk = chunk)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
totalTaskDist <- label_parallel_runs(matrix = totalTaskDist, n = n, simulation = sim, chunk = chunk)
# Create thresh table
threshMat <- label_parallel_runs(matrix = threshMat, n = n, simulation = sim, chunk = chunk)
# Create rank correlation table
taskCorr$replicate <- sim
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_thresh[[sim]]      <- threshMat
ens_graphs[[sim]]      <- g_tot / gens
ens_taskCorr[[sim]]  <- taskCorr
}
n <- run_in_parallel[k, 1]
chunk <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_taskDist    <- list()
ens_entropy     <- list()
ens_thresh      <- list()
ens_graphs      <- list()
ens_taskCorr  <- list()
# Run Simulations
for (sim in 1:chunk_size) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
taskStep <- list()
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
# Create time step for correlation
if (t %% corrStep == 0) {
# Get tasks performance in correlation step
X_step <- X_tot - X_prevTot
# Add to ensemble list of task steps
taskStep[[t / corrStep]] <- X_step
# Calculate rank correlation if it is not the first step
if(sum(X_prev) != 0) {
# Normalize
stepNorm <- X_step / rowSums(X_step)
prevNorm <- X_prev / rowSums(X_prev)
# Calculate ranks
step_ranks <- calculateTaskRank(TaskStepMat = X_step)
prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)
# Calculate Correlation
rankCorr <- cor(prev_ranks, step_ranks, method = "spearman")
# Put in list
taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)
names(taskCorr)[(t / corrStep) - 1] <- paste0("Gen", t)
}
# Update previous step total matrix
X_prevTot <- X_tot
# Update previous step total matrix
X_prev <- X_step
}
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- label_parallel_runs(matrix = entropy, n = n, simulation = sim, chunk = chunk)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
totalTaskDist <- label_parallel_runs(matrix = totalTaskDist, n = n, simulation = sim, chunk = chunk)
# Create thresh table
threshMat <- label_parallel_runs(matrix = threshMat, n = n, simulation = sim, chunk = chunk)
# Create rank correlation table
taskCorr$replicate <- sim
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_thresh[[sim]]      <- threshMat
ens_graphs[[sim]]      <- g_tot / gens
ens_taskCorr[[sim]]  <- taskCorr
}
# Bind task correlation data
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
replicate <- runs[nrow(runs), ]
replicate <- unique(replicate)
runs <- runs[-nrow(runs), ]
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
runMean <- cbind(runMean, replicate)
colnames(runMean) <- c(paste0("Task", 1:m), "replicate")
return(runMean)
})
runCorrs <- do.call("rbind", runCorrs)
runCorrs$n <- n
runnCorrs$chunk <- chunk
runCorrs
# Bind task correlation data
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
replicate <- runs[nrow(runs), ]
replicate <- unique(replicate)
runs <- runs[-nrow(runs), ]
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
runMean <- cbind(runMean, replicate)
colnames(runMean) <- c(paste0("Task", 1:m), "replicate")
return(runMean)
})
runCorrs
runCorrs <- do.call("rbind", runCorrs)
View(runCorrs)
runCorrs <- as.data.frame(do.call("rbind", runCorrs))
# Bind task correlation data
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
replicate <- runs[nrow(runs), ]
replicate <- unique(replicate)
runs <- runs[-nrow(runs), ]
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
runMean <- cbind(runMean, replicate)
colnames(runMean) <- c(paste0("Task", 1:m), "replicate")
return(runMean)
})
runCorrs <- as.data.frame(do.call("rbind", runCorrs))
runCorrs$n <- n
runnCorrs$chunk <- chunk
View(runCorrs)
