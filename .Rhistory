assort <- assort$r
to_retun <- data.frame(Assortativity = assort)
# return
return(to_retun)
})
size_data <- do.call("rbind", size_graph)
size_data <- size_data %>%
mutate(parameter_value = as.numeric(gsub(long_runs[run], pattern = ".*Beta([\\.0-9]+)", replacement = "\\1", perl = T)),
parameter = "Beta") %>%
group_by(parameter_value,
parameter) %>%
summarise(Assort_mean = mean(Assortativity),
Assort_SD = sd(Assortativity))
return(size_data)
})
long_run_assort <- do.call('rbind', long_run_assort)
# Plot
assort_data_beta <- assort_data %>%
filter(parameter == "Beta")
gg_assort_beta <- ggplot(data = assort_data_beta, aes(x = parameter_value, y = Assort_mean,
colour = parameter, group = parameter, fill = parameter)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
# Regular data
geom_errorbar(aes(ymin = Assort_mean - Assort_SD, ymax = Assort_mean + Assort_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
# long run data
geom_errorbar(data = long_run_assort,
aes(ymin = Assort_mean - Assort_SD, ymax = Assort_mean + Assort_SD),
width = 0,
size = 0.3) +
geom_point(data = long_run_assort,
size = 0.8, shape = 21, fill = "red") +
# Other stuff
scale_color_manual(name = "Threshold",
# values = c("#878787", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_fill_manual(name = "Threshold",
# values = c("#ffffff", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_x_continuous(breaks = seq(1, 1.25, 0.05)) +
scale_y_continuous(breaks = seq(-0.04, 0.1, 0.02), limits = c(-0.02, 0.062)) +
xlab(expression(paste("Interaction bias (", italic(beta), ")"))) +
ylab("Assortativity") +
theme_ctokita() +
theme(legend.position = "none",
axis.title.y = element_text(vjust = -1.5))
gg_assort_beta
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
library(RColorBrewer)
library(scales)
##########################################################
# Modularity
##########################################################
rm(list = ls())
source("scripts/util/__Util__MASTER.R")
p <- 1 #prob of interact
runs <- c("Sigma0-Epsilon0.1_BetaSweep",
"Sigma0-Beta1.1_EpsSweep")
run_names <- c("Beta", "Epsilon")
modularity <- lapply(1:length(runs), function(run) {
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
parameter_value <- gsub(".*/([\\.0-9]+).Rdata", "\\1", x = files, perl = T)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs[[j]]
diag(this_graph) <- 0
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
clust_coeff <- transitivity(graph = g, type = "weighted", weights = E(g)$weight)
# return
replicate_row <- data.frame(parameter_value = as.numeric(parameter_value[i]),
Modularity = mod,
ClustCoeff =  mean(clust_coeff, na.rm = TRUE))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$parameter <- run_names[run]
return(interaction_info)
})
# Bind
mod_data <- do.call("rbind", modularity)
mod_data <- mod_data %>%
group_by(parameter, parameter_value) %>%
summarise(Modul_mean = mean(Modularity),
Modul_SD = sd(Modularity),
Modul_SE = sd(Modularity)/length(Modularity))
# Specific parameters (all run for 10x longer, i.e., 500k time steps)
long_runs <- c("Sigma0-Epsilon0.1-Beta1.025",
"Sigma0-Epsilon0.1-Beta1.05",
"Sigma0-Epsilon0.1-Beta1.075")
long_run_mod <- lapply(1:length(long_runs), function(run) {
# Load social networks
load(paste0("output/Rdata/_ProcessedData/_LongSims/Graphs/", long_runs[run], ".Rdata"))
# Load threshold matrices
load(paste0("output/Rdata/_ProcessedData/_LongSims/Thresh/", long_runs[run], ".Rdata"))
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs_data), function(j) {
# Format: set diagonal, rescale, and make adj matrix
this_graph <- graphs_data[[j]]
diag(this_graph) <- 0
g <- graph_from_adjacency_matrix(this_graph, mode = "undirected", weighted = TRUE)
g_clust <- cluster_fast_greedy(g, weights = E(g)$weight)
# g_membership <- membership(g_clust)
# mod <- modularity(g, membership = g_membership, weights = E(g)$weight)
mod <- modularity(g_clust)
clust_coeff <- transitivity(graph = g, type = "weighted", weights = E(g)$weight)
# return
replicate_row <- data.frame(Modularity = mod,
ClustCoeff =  mean(clust_coeff, na.rm = TRUE))
return(replicate_row)
})
size_data <- do.call("rbind", size_graph)
size_data <- size_data %>%
mutate(parameter_value = as.numeric(gsub(long_runs[run], pattern = ".*Beta([\\.0-9]+)", replacement = "\\1", perl = T)),
parameter = "Beta") %>%
group_by(parameter_value,
parameter) %>%
summarise(Modul_mean = mean(Modularity),
Modul_SD = sd(Modularity))
return(size_data)
})
long_run_mod <- do.call('rbind', long_run_mod)
# Plot
mod_data_beta <- mod_data %>%
filter(parameter == "Beta")
gg_mod_beta <- ggplot(mod_data_beta, aes(x = parameter_value, y = Modul_mean, colour = parameter, fill = parameter)) +
# normal data
geom_errorbar(aes(ymin = ifelse(Modul_mean - Modul_SD < 0, 0, Modul_mean - Modul_SD), ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
# long run data
geom_errorbar(data = long_run_mod,
aes(ymin = ifelse(Modul_mean - Modul_SD < 0, 0, Modul_mean - Modul_SD), ymax = Modul_mean + Modul_SD),
width = 0,
size = 0.3) +
geom_point(data = long_run_mod,
size = 0.8, shape = 21, fill = "red") +
# Other stuff
scale_color_manual(name = "Threshold",
# values = c("#878787", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_fill_manual(name = "Threshold",
# values = c("#ffffff", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_x_continuous(breaks = seq(1, 1.5, 0.05)) +
scale_y_continuous(breaks = seq(0, 0.03, 0.01), limits = c(-0.0002, 0.031)) +
xlab(expression(paste("Interaction bias (", italic(beta), ")"))) +
ylab("Modularity") +
theme_ctokita() +
theme(legend.position = "none",
legend.key.height = unit(0.5, "line"))
gg_mod_beta
ggsave(gg_mod_beta, file = "output/LongSims/modularity_plots.png", width = 45, height = 45, units = "mm", dpi = 400)
source("scripts/util/__Util__MASTER.R")
p <- 1 #prob of interact
runs <- c("Sigma0-Epsilon0.1_BetaSweep",
"Sigma0-Beta1.1_EpsSweep")
run_names <- c("Beta", "Epsilon")
###################
# Assortment coefficient from Newman 2003
###################
library(assortnet)
library(gridExtra)
network_assort <- lapply(1:length(runs), function(run) {
print(runs[run])
# Load social networks
files <- list.files(paste0("output/Rdata/_ProcessedData/Graphs/", runs[run], "/"), full.names = TRUE)
parameter_value <- gsub(".*/([\\.0-9]+).Rdata", "\\1", x = files, perl = T)
soc_networks <- list()
for (file in 1:length(files)) {
load(files[file])
soc_networks[[file]] <- listed_data
}
# Load threshold matrices
files <- list.files(paste0("output/Rdata/_ProcessedData/Thresh/", runs[run], "/"), full.names = TRUE)
thresh_data <- list()
for (file in 1:length(files)) {
load(files[file])
thresh_data[[file]] <- listed_data
}
# Loop through individual graphs
interaction_info <- lapply(1:length(soc_networks), function(i) {
# Get graphs
graphs <- soc_networks[[i]]
replicates <- length(graphs)
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs[[j]]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[i]][j])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Calculate assortmnet
assort <- assortment.continuous(graph = this_graph, vertex_values = thresh$ThreshBias, weighted = T)
assort <- assort$r
to_retun <- data.frame(parameter_value = as.numeric(parameter_value[i]), Assortativity = assort)
# return
return(to_retun)
})
#Calculate baseline probability of interaction
size_graph <- do.call("rbind", size_graph)
})
# Bind and return
interaction_info <- do.call("rbind", interaction_info)
interaction_info$parameter <- run_names[run]
return(interaction_info)
})
# Bind
assort_data <- do.call('rbind', network_assort)
assort_data <- assort_data %>%
mutate(Assortativity = ifelse(Assortativity == -1, NA, Assortativity)) %>% #remove huge outlier
group_by(parameter, parameter_value) %>%
summarise(Assort_mean = mean(Assortativity, na.rm = T),
Assort_SD = sd(Assortativity, na.rm = T),
Assort_SE = sd(Assortativity, na.rm = T)/length(Assortativity))
# Specific parameters (all run for 10x longer, i.e., 500k time steps)
long_runs <- c("Sigma0-Epsilon0.1-Beta1.025",
"Sigma0-Epsilon0.1-Beta1.05",
"Sigma0-Epsilon0.1-Beta1.075")
long_run_assort <- lapply(1:length(long_runs), function(run) {
# Load social networks
load(paste0("output/Rdata/_ProcessedData/_LongSims/Graphs/", long_runs[run], ".Rdata"))
# Load threshold matrices
load(paste0("output/Rdata/_ProcessedData/_LongSims/Thresh/", long_runs[run], ".Rdata"))
# For each each compute interaction matrix
# Get graph and make adjacency matrix
size_graph <- lapply(1:length(graphs_data), function(j) {
# Get graph and calculate threshold differences
this_graph <- graphs_data[[j]]
diag(this_graph) <- 0
thresh <- as.data.frame(thresh_data[[j]])
thresh$ThreshBias <- thresh$Thresh1 - thresh$Thresh2
# Calculate assortmnet
assort <- assortment.continuous(graph = this_graph, vertex_values = thresh$ThreshBias, weighted = T)
assort <- assort$r
to_retun <- data.frame(Assortativity = assort)
# return
return(to_retun)
})
size_data <- do.call("rbind", size_graph)
size_data <- size_data %>%
mutate(parameter_value = as.numeric(gsub(long_runs[run], pattern = ".*Beta([\\.0-9]+)", replacement = "\\1", perl = T)),
parameter = "Beta") %>%
group_by(parameter_value,
parameter) %>%
summarise(Assort_mean = mean(Assortativity),
Assort_SD = sd(Assortativity))
return(size_data)
})
long_run_assort <- do.call('rbind', long_run_assort)
# Plot
assort_data_beta <- assort_data %>%
filter(parameter == "Beta")
gg_assort_beta <- ggplot(data = assort_data_beta, aes(x = parameter_value, y = Assort_mean,
colour = parameter, group = parameter, fill = parameter)) +
geom_hline(yintercept = 0, color = "black", size = 0.3, linetype = "dotted") +
# Regular data
geom_errorbar(aes(ymin = Assort_mean - Assort_SD, ymax = Assort_mean + Assort_SD),
width = 0,
size = 0.3) +
geom_point(size = 0.8, shape = 21) +
# long run data
geom_errorbar(data = long_run_assort,
aes(ymin = Assort_mean - Assort_SD, ymax = Assort_mean + Assort_SD),
width = 0,
size = 0.3) +
geom_point(data = long_run_assort,
size = 0.8, shape = 21, fill = "red") +
# Other stuff
scale_color_manual(name = "Threshold",
# values = c("#878787", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_fill_manual(name = "Threshold",
# values = c("#ffffff", "#4d4d4d")) +
values = c("#4d4d4d")) +
scale_x_continuous(breaks = seq(1, 1.25, 0.05)) +
scale_y_continuous(breaks = seq(-0.04, 0.1, 0.02), limits = c(-0.02, 0.062)) +
xlab(expression(paste("Interaction bias (", italic(beta), ")"))) +
ylab("Assortativity") +
theme_ctokita() +
theme(legend.position = "none",
axis.title.y = element_text(vjust = -1.5))
gg_assort_beta
ggsave(gg_assort_beta, file = "output/LongSims/assortativity_plots.png", width = 45, height = 45, units = "mm", dpi = 400)
rm(list = ls())
####################
# Source necessary scripts/libraries
####################
source("scripts/util/__Util__MASTER.R")
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- c(80) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 50000 #number of generations to run simulation
reps           <- 1 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- rep(50, m) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- rep(0, m) #intital vector of stimuli
deltas         <- rep(0.8, m) #vector of stimuli increase rates
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
thresh_max     <- 100
# Social Network Parameters
p              <- 1 #baseline probablity of initiating an interaction per time step
epsilon        <- 0.1 #relative weighting of social interactions for adjusting thresholds
beta           <- 1.1 #probability of interacting with individual in same state relative to others
####################
# Run ensemble simulation
####################
# Prep meta-lists for collection of group size simulations
groups_taskDist    <- list()
groups_taskTally   <- list()
groups_stim        <- list()
groups_thresh      <- list()
groups_entropy     <- list()
groups_thresh1Time <- list()
groups_thresh2Time <- list()
groups_graphs      <- list()
# Loop through group sizes
for (i in 1:length(Ns)) {
# Set group size
n <- Ns[i]
# Prep lists for collection of simulation outputs from this group size
ens_taskDist    <- list()
ens_taskTally   <- list()
ens_entropy     <- list()
ens_stim        <- list()
ens_thresh      <- list()
ens_thresh1Time <- list()
ens_thresh2Time <- list()
ens_graphs      <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
# Prep lists for data collection within simulation
taskTally <- list()
thresh1time <- list()
thresh2time <- list()
thresh1time[[1]] <- threshMat[ ,1]
thresh2time[[1]] <- threshMat[ ,2]
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = thresh_max)
# Capture threshold values
thresh1time[[t + 1]] <- threshMat[,1]
thresh2time[[t + 1]] <- threshMat[,2]
# Update total task performance profile
X_tot <- X_tot + X_g
# Capture current task performance tally
tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)
colnames(tally) <- c("t", colnames(X_g))
taskTally[[t]] <- tally
}
####################
# Post run calculations
####################
# Bind together task tally
col_names <- colnames(taskTally[[1]])
taskTally <- matrix(unlist(taskTally),
ncol = length(taskTally[[1]]),
byrow = TRUE,
dimnames = list(c(NULL), c(col_names)))
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
# Calculate total task distribution
totalTaskDist <- X_tot / gens
# Create tasktally table
stimMat <- cbind(stimMat, 0:(nrow(stimMat) - 1))
colnames(stimMat)[ncol(stimMat)] <- "t"
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]    <- totalTaskDist
ens_entropy[[sim]]     <- entropy
ens_taskTally[[sim]]   <- taskTally
ens_stim[[sim]]        <- stimMat
ens_thresh[[sim]]      <- threshMat
ens_thresh1Time[[sim]] <- thresh1time
ens_thresh2Time[[sim]] <- thresh2time
ens_graphs[[sim]]      <- g_tot / gens
}
# Add to list of lists
groups_taskDist[[i]]    <- ens_taskDist
groups_taskTally[[i]]   <- ens_taskTally
groups_stim[[i]]        <- ens_stim
groups_thresh[[i]]      <- ens_thresh
groups_entropy[[i]]     <- ens_entropy
groups_thresh1Time[[i]] <- ens_thresh1Time
groups_thresh2Time[[i]] <- ens_thresh2Time
groups_graphs[[i]]      <- ens_graphs
}
thresh_time <- do.call('rbind', thresh1time)
thresh_time <- as.data.frame(thresh_time)
thresh_time <- thresh_time %>%
mutate(t = 1:nrow(.)) %>%
gather(., Id, Threshold, -t)
# ------------------------------ Thresholds over time  ------------------------------
####################
# Load normal threshold limit (epsilon = 0.4, beta = 1.1)
####################
library(scales)
gg_threshtime_100 <- ggplot(thresh_time, aes(x = t, y = Threshold, group = Id)) +
geom_line(size = 0.1, alpha = 0.1, colour = "#1f78b4") +
scale_x_continuous(name = expression(paste("Time step (", italic(t), ")")),
breaks = seq(0, 50000, 10000),
labels = c("", "10,000", "", "30,000", "", "50,000"),
expand = c(0, 0)) +
scale_y_continuous(name = expression(paste("Task 1 threshold (", italic(theta[i1,t]), ")")),
limits = c(0, 100),
breaks = seq(0, 100, 50)) +
theme_ctokita() +
theme(axis.title.y = element_blank(),
axis.text.x = element_text(hjust = 0.7))
gg_threshtime_100
gg_threshtime_100 <- ggplot(thresh_time, aes(x = t, y = Threshold, group = Id)) +
geom_line(size = 0.1, alpha = 0.1, colour = "#1f78b4") +
scale_x_continuous(name = expression(paste("Time step (", italic(t), ")")),
breaks = seq(0, 50000, 10000),
labels = c("", "10,000", "", "30,000", "", "50,000"),
expand = c(0, 0)) +
scale_y_continuous(name = expression(paste("Task 1 threshold (", italic(theta[i1,t]), ")")),
limits = c(0, 100),
breaks = seq(0, 100, 50)) +
theme_ctokita() +
theme(#axis.title.y = element_blank(),
axis.text.x = element_text(hjust = 0.7))
gg_threshtime_100
ggsave(gg_threshtime_100, file = "output/ThresholdTime/ThresholdLimits/ExampleforPPT.png", height = 45, width = 45, units = "mm", dpi = 500)
ggsave(gg_threshtime_100, file = "output/ThresholdTime/ThresholdLimits/ExampleforPPT.png", height = 26, width = 45, units = "mm", dpi = 500)
ggsave(gg_threshtime_100, file = "output/ThresholdTime/ThresholdLimits/ExampleforPPT.png", height = 34, width = 45, units = "mm", dpi = 500)
