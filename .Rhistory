sfExportAll()
sfLibrary(dplyr)
sfLibrary(reshape2)
sfLibrary(igraph)
sfLibrary(ggplot2)
sfLibrary(msm)
sfLibrary(gtools)
sfLibrary(snowfall)
sfLibrary(tidyr)
sfLibrary(stringr)
sfClusterSetupRNGstream(seed = 323)
####################
# Run ensemble simulation
####################
# Loop through group size (and chucnks)
parallel_simulations <- sfLapply(1:nrow(run_in_parallel), function(k) {
# Set group size
n <- run_in_parallel[k, 1]
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
# Bind together and summarise
entropy_sum <- do.call("rbind", ens_entropy)
entropy_sum <- entropy_sum %>%
group_by(n, beta) %>%
summarise(Dsym_mean = mean(Dsym),
Dysm_SD = sd(Dsym),
Dtask_mean = mean(Dtask),
Dtask_SD = sd(Dtask),
Dind_mean = mean(Dind),
Dind_SD = sd(Dind))
entropy_sum <- as.data.frame(entropy_sum)
return(entropy_sum)
sys.sleep(1)
})
sfStop()
# Set group size
n <- run_in_parallel[k, 1]
k = 1
# Set group size
n <- run_in_parallel[k, 1]
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
# Bind together and summarise
entropy_sum <- do.call("rbind", ens_entropy)
View(entropy_sum)
k = 2
# Set group size
n <- run_in_parallel[k, 1]
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
betas          <- seq(1, 1, 0.005) #probability of interacting with individual in same state relative to others
betas
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
betas          <- seq(1, 1.01, 0.005) #probability of interacting with individual in same state relative to others
n = 4
k = 1
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, beta = betas)
run_in_parallel <- run_in_parallel %>%
arrange(n)
beta <- run_in_parallel[k, 2]
# Prep lists for collection of simulation outputs from this group size
ens_entropy     <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- matrix(data = rep(0, n * m), ncol = m)
# Seed task (external) stimuli
stimMat <- seed_stimuls(intitial_stim = InitialStim,
gens = gens)
# Seed internal thresholds
threshMat <- seed_thresholds(n = n,
m = m,
threshold_means = ThreshM,
threshold_sds = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Create cumulative adjacency matrix
g_tot <-  matrix(data = rep(0, n * n), ncol = n)
colnames(g_tot) <- paste0("v-", 1:n)
rownames(g_tot) <- paste0("v-", 1:n)
####################
# Simulate individual run
####################
# Run simulation
for (t in 1:gens) {
# Current timestep is actually t+1 in this formulation, because first row is timestep 0
# Update stimuli
stimMat <- update_stim(stim_matrix = stimMat,
deltas = deltas,
alpha = alpha,
state_matrix = X_g,
time_step = t)
# Calculate task demand based on global stimuli
P_g <- calc_determ_thresh(time_step        = t + 1, # first row is generation 0
threshold_matrix = threshMat,
stimulus_matrix  = stimMat)
# Update task performance
X_g <- update_task_performance(task_probs   = P_g,
state_matrix = X_g,
quit_prob    = quitP)
# Update social network (previously this was before probability/task update)
g_adj <- temporalNetwork(X_sub_g = X_g,
prob_interact = p,
bias = beta)
g_tot <- g_tot + g_adj
# Adjust thresholds
threshMat <- adjust_thresholds_social_capped(social_network = g_adj,
threshold_matrix = threshMat,
state_matrix = X_g,
epsilon = epsilon,
threshold_max = 2 * ThreshM[1])
# Update total task performance profile
X_tot <- X_tot + X_g
}
####################
# Post run calculations
####################
# Calculate Entropy
entropy <- as.data.frame(mutualEntropy(TotalStateMat = X_tot))
entropy$n <- n
entropy$beta <- beta
# Add entropy values to list
ens_entropy[[sim]] <- entropy
# Clean
rm(X_tot, stimMat, threshMat, g_tot, g_adj, P_g, X_g)
}
X_g
X_sub_g = X_g
prob_interact = p
bias = beta
dimension <- nrow(X_sub_g)
g_adj <- matrix(data = rep(0, dimension*dimension), ncol = dimension)
g_adj
# Determine if interact
interact <- sample(x = c(1, 0), size = 1, prob = c(prob_interact, 1 - prob_interact))
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
i = 1
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
# catch for if there is only two individuals
if (length(baseline_prob) == 1) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
potential
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
connection <- sample(x = potential, size = 1, prob = baseline_prob)
potential
baseline_prob
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
baseline_prob
# catch for if there is only two individuals
if (length(baseline_prob) == 1) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
baseline_prob
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
task
X_g
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
i
colNanmes(same)
names(same)
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[!i , task] == 1)
same
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
connection <- sample(x = potential, size = 1, prob = baseline_prob)
baseline_prob
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
baseline_prob <- baseline_prob[-i]
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# catch for if there is only two individuals
if (length(baseline_prob) == 2) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
same
baseline_prob[same] <- baseline_prob[same] * bias
baseline_prob
baseline_prob[same] <- baseline_prob[same] * bias
bias
bias = 2
baseline_prob[same] <- baseline_prob[same] * bias
baseline_prob
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
potential
baseline_prob
connection <- sample(x = potential, size = 1, prob = baseline_prob)
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# remove self
potential <- potential[-i]
# get task performance of individual
task <- which(X_sub_g[i, ] == 1)
# set up list of potential connections
potential <- seq(1:dimension)
baseline_prob <- rep(1, length(potential))
# catch for if there is only two individuals
if (length(baseline_prob) == 2) {
potential <- c(potential, 0)
baseline_prob <- c(baseline_prob, 0)
}
# find which individuals are perfoming same task and relatively weight probabilities
same <- which(X_sub_g[ , task] == 1)
baseline_prob[same] <- baseline_prob[same] * bias
potential <- potential[-i] # remove self
baseline_prob <- baseline_prob[-i] # remove self
connection <- sample(x = potential, size = 1, prob = baseline_prob)
source('~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/3a_BetaParaSweep.R', echo=TRUE)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(2, 100, 2) #vector of number of individuals to simulate
betas          <- seq(1, 1.25, 0.005) #probability of interacting with individual in same state relative to others
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, beta = betas)
run_in_parallel <- run_in_parallel %>%
arrange(n)
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 5) #vector of number of individuals to simulate
epsilons       <- seq(0, 0.5, 0.025) #relative weighting of social interactions for adjusting thresholds
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, epsilon = epsilons)
run_in_parallel <- run_in_parallel %>%
arrange(n)
epsilons       <- seq(0, 0.5, 0.01) #relative weighting of social interactions for adjusting thresholds
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- seq(5, 100, 2) #vector of number of individuals to simulate
####################
# Prep for Parallelization
####################
# Create parameter combinations for parallelization
run_in_parallel <- expand.grid(n = Ns, epsilon = epsilons)
run_in_parallel <- run_in_parallel %>%
arrange(n)
