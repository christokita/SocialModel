{
    "collab_server" : "",
    "contents" : "################################################################################\n#\n# Model incorporating both thresholds and network dynamics\n#\n################################################################################\n\nrm(list = ls())\nsource(\"scripts/__Util__MASTER.R\")\n\n####################\n# Set global variables\n####################\n# Initial paramters: Free to change\n# Base parameters\nNs             <- c(70) #vector of number of individuals to simulate\nm              <- 2 #number of tasks\ngens           <- 10000 #number of generations to run simulation \ncorrStep       <- 200 #number of time steps for calculation of correlation \nreps           <- 30 #number of replications per simulation (for ensemble)\n\n# Threshold Parameters\nThreshM        <- rep(10, m) #population threshold means \nThreshSD       <- ThreshM * 0.01 #population threshold standard deviations\nInitialStim    <- rep(0, m) #intital vector of stimuli\ndeltas         <- rep(0.6, m) #vector of stimuli increase rates  \nalpha          <- m #efficiency of task performance\nquitP          <- 0.2 #probability of quitting task once active\n\n# Social Network Parameters\nepsilon        <- 0.01 #relative weighting of social interactions for lowering thresholds #0.01 = epsilon = phi\nphi            <- 0.01 #default forgetting rate of thresholds\np              <- 0.1 #probability of interacting with individual in other states\nq              <- 1.1 #probability of interacting with individual in same state relative to others\n\n\n\n####################\n# Run simulation multiple times\n####################\n# Prep meta-lists for collection of group size simulations\ngroups_taskDist  <- list()\ngroups_taskCorr  <- list()\ngroups_taskStep  <- list()\ngroups_taskTally <- list()\ngroups_stim      <- list()\ngroups_entropy   <- list()\ngroups_graphs    <- list()\ngroups_specialization <- data.frame(NULL)\n\n# Loop through group sizes\nfor (i in 1:length(Ns)) {\n  # Set group size\n  n <- Ns[i]\n  \n  # Prep lists for collection of simulation outputs\n  ens_taskDist  <- list()\n  ens_taskCorr  <- list()\n  ens_taskStep  <- list()\n  ens_taskTally <- list()\n  ens_entropy   <- list()\n  ens_stim      <- list()\n  ens_graphs    <- list()\n  \n  # Run Simulations\n  for (sim in 1:reps) {\n    \n    ####################\n    # Seed structures and intial matrices\n    ####################\n\n    # Set initial probability matrix (P_g)\n    P_g <- matrix(data = rep(0, n * m), ncol = m)\n    \n    # Seed task (external) stimuli\n    stimMat <- seedStimuls(InitialSVector = InitialStim, \n                           gens = gens)\n    \n    # Seed internal thresholds\n    threshMat <- seedThresholds(n = n, \n                                m = m, \n                                ThresholdMeans = ThreshM, \n                                ThresholdSDs = ThreshSD)\n    \n    # Start task performance\n    X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))\n    \n    # Create cumulative task performance matrix\n    X_tot <- X_g\n    \n    # Create cumulative adjacency matrix\n    g_tot <-  matrix(data = rep(0, n * n), ncol = n)\n    colnames(g_tot) <- paste0(\"v-\", 1:n)\n    rownames(g_tot) <- paste0(\"v-\", 1:n)\n    \n    # Prep correlation step matrix\n    X_prev <- matrix(data = rep(0, n * m), ncol = m)\n    X_prevTot <- matrix(data = rep(0, n * m), ncol = m)\n    taskCorr <- list()\n    taskStep <- list()\n    taskTally <- list()\n    taskOverTime  <- matrix(nrow = 0, ncol = n)\n    \n    ####################\n    # Simulate\n    ####################\n    # Run simulation\n    for (t in 1:gens) {\n      # Update stimuli\n      for (j in 1:ncol(stimMat)) {\n        # update stim\n        stimMat[t + 1, j] <- globalStimUpdate(stimulus = stimMat[t, j],\n                                              delta = deltas[j], \n                                              alpha = alpha, \n                                              Ni = sum(X_g[ , j]), \n                                              n = n)\n      }\n      # Update social network\n      g_adj <- temporalNetwork(X_sub_g = X_g,\n                               p = p,\n                               bias = q)\n      g_tot <- g_tot + g_adj\n      # Calculate information on neighbors performing tasks\n      L_g <- calcSocialInfo(SocialNetwork = g_adj,\n                            X_sub_g = X_g)\n      # Calculate task demand based on global stimuli\n      P_g <- calcThresholdDetermMat(TimeStep = t + 1, # first row is generation 0\n                                    ThresholdMatrix = threshMat, \n                                    StimulusMatrix = stimMat)\n      # Update task performance\n      X_g <- updateTaskPerformance(P_sub_g    = P_g,\n                                   TaskMat    = X_g,\n                                   QuitProb   = quitP)\n      # Adjust thresholds\n      threshMat <- adjustThresholdsSocial(SocialNetwork = g_adj,\n                                          ThresholdMatrix = threshMat, \n                                          X_sub_g = X_g, \n                                          epsilon = epsilon, \n                                          phi = phi)\n      # Note which task is being peformed\n      taskPerf <- matrix(nrow = 1, ncol = n)\n      for (i in 1:nrow(X_g)) {\n        task <- unname(which(X_g[i, ] == 1))\n        if (length(task) == 0) {\n          task <- 0\n        }\n        taskPerf[i] <- task\n      }\n      colnames(taskPerf) <- row.names(X_g)\n      taskOverTime <- rbind(taskOverTime, taskPerf)\n      \n      # Capture current task performance tally\n      tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)\n      colnames(tally) <- c(\"t\", colnames(X_g))\n      tally <- transform(tally, Inactive = n - sum(X_g), n = n, replicate = sim)\n      taskTally[[t]] <- tally\n      \n      # Update total task performance profile\n      X_tot <- X_tot + X_g\n      \n      # Create time step for correlation\n      if (t %% corrStep == 0) {\n        # Get tasks performance in correlation step\n        X_step <- X_tot - X_prevTot\n        # Add to ensemble list of task steps\n        taskStep[[t / corrStep]] <- X_step\n        # Calculate rank correlation if it is not the first step\n        if(sum(X_prev) != 0) {\n          # Normalize\n          stepNorm <- X_step / rowSums(X_step)\n          prevNorm <- X_prev / rowSums(X_prev)\n          # Calculate ranks\n          step_ranks <- calculateTaskRank(TaskStepMat = X_step)\n          prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)\n          # Calculate Correlation\n          rankCorr <- cor(prev_ranks, step_ranks, method = \"spearman\")\n          # Put in list\n          taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)\n          names(taskCorr)[(t / corrStep) - 1] <- paste0(\"Gen\", t)\n        }\n        # Update previous step total matrix\n        X_prevTot <- X_tot\n        # Update previous step total matrix\n        X_prev <- X_step\n      }\n    }\n    \n    # Calculate specialization of task performance \n    # from Gautrais et al. (2002)\n    for (col in 1:ncol(taskOverTime)) {\n      # Grab column of individual\n      t_prof <- taskOverTime[ , col ]\n      # Remove inactivity\n      t_prof <- paste(t_prof, collapse = \"\")\n      # Calculate transitions\n      t_prof <- gsub(\"1+\", \"1\", t_prof)\n      t_prof <- gsub(\"2+\", \"2\", t_prof)\n      t_prof <- gsub(\"0+\", \"\", t_prof)\n      t_prof <- as.numeric(unlist(strsplit(as.character(t_prof), \"\")))\n      transitions <- lapply(2:length(t_prof), function(entry) {\n        a <- t_prof[entry] != t_prof[entry - 1]\n      })\n      C_i <- sum(unlist(transitions))\n      C_i <- C_i / (length(t_prof) - 1)\n      # Calulate specialization\n      F_i <- 1 - m * C_i\n      to_return <- data.frame(individual = paste0(\"v-\", col), \n                              n = n,\n                              replicate = sim,\n                              TransSpec = F_i)\n      groups_specialization <- rbind(groups_specialization, to_return)\n    }\n    \n    # Calculate Entropy\n    entropy <- mutualEntropy(TotalStateMat = X_tot)\n    entropy <- transform(entropy, n = n, replicate = sim)\n    \n    # Calculate total task distribution\n    # totalTaskDist <- X_tot / rowSums(X_tot)\n    totalTaskDist <- X_tot / gens\n    totalTaskDist <- transform(totalTaskDist, Inactive = gens - rowSums(X_tot), n = n, replicate = sim)\n    \n    # Create tasktally table\n    taskTally <- do.call(\"rbind\", taskTally)\n    \n    # Create tasktally table\n    stimMat <- transform(stimMat, n = n, replicate = sim)\n    \n    # Create tasktally table\n    taskCorr <- transform(taskCorr, replicate = sim)\n    \n    # Add total task distributions, entropy values, and graphs to lists\n    ens_taskDist[[sim]]  <- totalTaskDist\n    ens_entropy[[sim]]   <- entropy\n    ens_taskCorr[[sim]]  <- taskCorr\n    ens_taskTally[[sim]] <- taskTally\n    ens_taskStep[[sim]]  <- taskStep\n    ens_stim[[sim]]      <- stimMat\n    ens_graphs[[sim]]    <- g_tot / gens\n    \n    # Print simulation completed\n    print(paste0(\"DONE: N = \", n, \", Simulation \", sim))\n  }\n  \n  # Calculate mean correlation for each n\n  runCorrs <- lapply(ens_taskCorr, function(x) {\n    # Unlist\n    runs <- do.call(\"rbind\", x)\n    replicate <- runs[nrow(runs), ]\n    replicate <- unique(replicate)\n    runs <- runs[-nrow(runs), ]\n    # Calculate mean\n    runMean <- matrix(data = rep(NA, m), ncol =  m)\n    for (column in 1:m) {\n      runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)\n    }\n    runMean <- cbind(runMean, replicate)\n    colnames(runMean) <- c(paste0(\"Task\", 1:m), \"replicate\")\n    return(runMean)\n  })\n  runCorrs <- do.call(\"rbind\", runCorrs)\n  runCorrs <- transform(runCorrs, n = n)\n  \n  # Add to list of lists\n  groups_taskDist[[i]]  <- ens_taskDist\n  groups_taskCorr[[i]]  <- runCorrs\n  groups_taskStep[[i]]  <- ens_taskStep\n  groups_taskTally[[i]] <- ens_taskTally\n  groups_stim[[i]]      <- ens_stim\n  groups_entropy[[i]]   <- ens_entropy\n  groups_graphs[[i]]    <- ens_graphs\n  \n}\n\n# trim out correlations for group size 1\nif(1 %in% Ns) {\n  groups_taskCorr <- groups_taskCorr[-1]\n}\n\nfilename <- \"Sigma001-Eps001-Phi001-ConnectP01-Bias1.1_LargerGroups\"\n\nsave(groups_entropy, groups_stim, groups_taskCorr, groups_taskDist, groups_graphs,\n     groups_taskStep, groups_taskTally, groups_specialization,\n     file = paste0(\"output/Rdata/\", filename, \".Rdata\"))\n\n# qplot(threshMat[,1], threshMat[,2]) + \n#   scale_color_gradient2(low = \"red\", mid = \"yellow\", high = \"blue\", midpoint = (max(threshMat) + min(threshMat)) / 2) + \n#   theme_bw()\n# qplot(X_tot[,1], X_tot[,2], col = X_tot[,3]) + \n#   scale_color_gradient2(low = \"purple\", mid = \"grey\", high = \"green\", midpoint = (max(X_tot) + min(X_tot)) / 2) + \n#   theme_bw()\n# \n# plot(stimMat[,1], type = \"l\")\n",
    "created" : 1511363428183.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "112133182",
    "id" : "2D058777",
    "lastKnownWriteTime" : 1511375509,
    "last_content_update" : 1511375509345,
    "path" : "~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/1_SocThreshModel.R",
    "project_path" : "scripts/1_SocThreshModel.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}