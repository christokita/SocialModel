{
    "collab_server" : "",
    "contents" : "################################################################################\n#\n# Model incorporating both thresholds and network dynamics\n#\n################################################################################\n\nrm(list = ls())\nsource(\"scripts/__Util__MASTER.R\")\n\n####################\n# Set global variables\n####################\n# Initial paramters: Free to change\n# Base parameters\nNs             <- c(32, 100) #vector of number of individuals to simulate\nm              <- 2 #number of tasks\ngens           <- 10000 #number of generations to run simulation \ncorrStep       <- 200 #number of time steps for calculation of correlation \nreps           <- 100 #number of replications per simulation (for ensemble) !!Change!!\n\n# Threshold Parameters\nThreshM        <- c(10, 10) #population threshold means \nThreshSD       <- ThreshM * 0.1 #population threshold standard deviations !!Change!!\nInitialStim    <- c(0, 0) #intital vector of stimuli\nStimRates      <- c(0.6, 0.6) #vector of stimuli increase rates  \nthreshSlope    <- 7 #exponent parameter for threshold curve shape  \nalpha          <- m #efficiency of task performance\nquitP          <- 0.2 #probability of quitting task once active\n\n# Social Network Parameters\np              <- 0 #probability of interacting with individual in other states\nq              <- 1 #probability of interacting with individual in same state relative to others\n\n\nfilename <- \"FixedDelta06Sigma01Eta7LargerSizes\"\n\n\n####################\n# Run simulation multiple times\n####################\n# Prep meta-lists for collection of group size simulations\ngroups_taskDist  <- list()\ngroups_taskCorr  <- list()\ngroups_taskStep  <- list()\ngroups_taskTally <- list()\ngroups_stim      <- list()\ngroups_entropy   <- list()\ngroups_specialization <- data.frame(NULL)\n\n# Loop through group sizes\nfor (i in 1:length(Ns)) {\n  # Set group size\n  n <- Ns[i]\n  \n  # Prep lists for collection of simulation outputs\n  ens_taskDist  <- list()\n  ens_taskCorr  <- list()\n  ens_taskStep  <- list()\n  ens_taskTally <- list()\n  ens_entropy   <- list()\n  ens_stim      <- list()\n  \n  # Run Simulations\n  for (sim in 1:reps) {\n    \n    ####################\n    # Seed structures and intial matrices\n    ####################\n\n    # Set initial probability matrix (P_g)\n    P_g <- initiateProbMatrix(n = n, m = m)\n    \n    # Seed task (external) stimuli\n    stimMat <- seedStimuls(InitialSVector = InitialStim, \n                           RateVector = StimRates, \n                           gens = gens)\n    \n    # Seed internal thresholds\n    threshMat <- seedThresholds(n = n, \n                                m = m, \n                                ThresholdMeans = ThreshM, \n                                ThresholdSDs = ThreshSD)\n    \n    # Start task performance\n    X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))\n    \n    # Create cumulative task performance matrix\n    X_tot <- X_g\n    \n    # Prep correlation step matrix\n    X_prev <- matrix(data = rep(0, n * m), ncol = m)\n    X_prevTot <- matrix(data = rep(0, n * m), ncol = m)\n    taskCorr <- list()\n    taskStep <- list()\n    taskTally <- list()\n    taskOverTime  <- matrix(nrow = 0, ncol = n)\n    \n    ####################\n    # Simulate\n    ####################\n    # Run simulation\n    for (t in 1:gens) {\n      # Update stimuli\n      for (j in 1:(ncol(stimMat)/2)) {\n        # update stim\n        stimMat[t + 1, j] <- globalStimUpdate(stimulus = stimMat[t, j],\n                                              delta = stimMat[t, j + m], \n                                              alpha = alpha, \n                                              Ni = sum(X_g[ , j]), \n                                              n = n)\n        # shift down delta (rate increases)\n        stimMat[t + 1, j + m] <- stimMat[t, j + m]\n      }\n      # Update social network\n      # g_adj <- temporalNetwork(X_sub_g = X_g,\n      #                          p = p, \n      #                          bias = q)\n      # Calculate task demand based on global stimuli\n      P_g <- calcThresholdProbMat(TimeStep = t + 1, # first row is generation 0\n                                  ThresholdMatrix = threshMat, \n                                  StimulusMatrix = stimMat, \n                                  nSlope = threshSlope)\n      # Update task performance\n      X_g <- updateTaskPerformance(P_sub_g    = P_g,\n                                   TaskMat    = X_g,\n                                   QuitProb   = quitP)\n      # Note which task is being peformed\n      taskPerf <- matrix(nrow = 1, ncol = n)\n      for (i in 1:nrow(X_g)) {\n        task <- unname(which(X_g[i, ] == 1))\n        if (length(task) == 0) {\n          task <- 0\n        }\n        taskPerf[i] <- task\n      }\n      colnames(taskPerf) <- row.names(X_g)\n      taskOverTime <- rbind(taskOverTime, taskPerf)\n      \n      # Capture current task performance tally\n      tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)\n      colnames(tally) <- c(\"t\", colnames(X_g))\n      tally <- transform(tally, Inactive = n - sum(X_g), n = n, replicate = sim)\n      taskTally[[t]] <- tally\n      \n      # Update total task performance profile\n      X_tot <- X_tot + X_g\n      \n      # Create time step for correlation\n      if (t %% corrStep == 0) {\n        # Get tasks performance in correlation step\n        X_step <- X_tot - X_prevTot\n        # Add to ensemble list of task steps\n        taskStep[[t / corrStep]] <- X_step\n        # Calculate rank correlation if it is not the first step\n        if(sum(X_prev) != 0) {\n          # Normalize\n          stepNorm <- X_step / rowSums(X_step)\n          prevNorm <- X_prev / rowSums(X_prev)\n          # Calculate ranks\n          step_ranks <- calculateTaskRank(TaskStepMat = X_step)\n          prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)\n          # Calculate Correlation\n          rankCorr <- cor(prev_ranks, step_ranks, method = \"spearman\")\n          # Put in list\n          taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)\n          names(taskCorr)[(t / corrStep) - 1] <- paste0(\"Gen\", t)\n        }\n        # Update previous step total matrix\n        X_prevTot <- X_tot\n        # Update previous step total matrix\n        X_prev <- X_step\n      }\n    }\n    \n    # Calculate specialization of task performance \n    # from Gautrais et al. (2002)\n    for (col in 1:ncol(taskOverTime)) {\n      # Grab column of individual\n      t_prof <- taskOverTime[ , col ]\n      # Remove inactivity\n      t_prof <- paste(t_prof, collapse = \"\")\n      # Calculate transitions\n      t_prof <- gsub(\"1+\", \"1\", t_prof)\n      t_prof <- gsub(\"2+\", \"2\", t_prof)\n      t_prof <- gsub(\"0+\", \"\", t_prof)\n      t_prof <- as.numeric(unlist(strsplit(as.character(t_prof), \"\")))\n      transitions <- lapply(2:length(t_prof), function(entry) {\n        a <- t_prof[entry] != t_prof[entry - 1]\n      })\n      C_i <- sum(unlist(transitions))\n      C_i <- C_i / (length(t_prof) - 1)\n      # Calulate specialization\n      F_i <- 1 - m * C_i\n      to_return <- data.frame(individual = paste0(\"v-\", col), \n                              n = n,\n                              replicate = sim,\n                              TransSpec = F_i)\n      groups_specialization <- rbind(groups_specialization, to_return)\n    }\n  \n    # Calculate Entropy\n    entropy <- mutualEntropy(TotalStateMat = X_tot)\n    entropy <- transform(entropy, n = n, replicate = sim)\n    \n    # Calculate total task distribution\n    # totalTaskDist <- X_tot / rowSums(X_tot)\n    totalTaskDist <- X_tot / gens\n    totalTaskDist <- transform(totalTaskDist, Inactive = gens - rowSums(X_tot), n = n, replicate = sim)\n    totalTaskDist$individual <- paste0(\"v-\", 1:nrow(totalTaskDist))\n    \n    # Create tasktally table\n    taskTally <- do.call(\"rbind\", taskTally)\n    \n    # Create tasktally table\n    stimMat <- transform(stimMat, n = n, replicate = sim)\n    \n    # Create tasktally table\n    taskCorr <- transform(taskCorr, replicate = sim)\n    \n    # Add total task distributions, entropy values, and graphs to lists\n    ens_taskDist[[sim]]  <- totalTaskDist\n    ens_entropy[[sim]]   <- entropy\n    ens_taskCorr[[sim]]  <- taskCorr\n    ens_taskTally[[sim]] <- taskTally\n    ens_taskStep[[sim]]  <- taskStep\n    ens_stim[[sim]]      <- stimMat\n    \n    # Print simulation completed\n    print(paste0(\"DONE: N = \", n, \", Simulation \", sim))\n  }\n  \n  # Calculate mean correlation for each n\n  runCorrs <- lapply(ens_taskCorr, function(x) {\n    # Unlist\n    runs <- do.call(\"rbind\", x)\n    replicate <- runs[nrow(runs), ]\n    replicate <- unique(replicate)\n    runs <- runs[-nrow(runs), ]\n    # Calculate mean\n    runMean <- matrix(data = rep(NA, m), ncol =  m)\n    for (column in 1:m) {\n      runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)\n    }\n    runMean <- cbind(runMean, replicate)\n    colnames(runMean) <- c(\"Task1\", \"Task2\", \"replicate\")\n    return(runMean)\n  })\n  runCorrs <- do.call(\"rbind\", runCorrs)\n  runCorrs <- transform(runCorrs, n = n)\n  \n  # Add to list of lists\n  groups_taskDist[[i]]  <- ens_taskDist\n  groups_taskCorr[[i]]  <- runCorrs\n  groups_taskStep[[i]]  <- ens_taskStep\n  groups_taskTally[[i]] <- ens_taskTally\n  groups_stim[[i]]      <- ens_stim\n  groups_entropy[[i]]   <- ens_entropy\n  \n}\n\n# trim out correlations for group size 1\nif(1 %in% Ns) {\n  groups_taskCorr <- groups_taskCorr[-1]\n}\n\n\n####################\n# Save all\n####################\nsave(groups_entropy, groups_stim, groups_taskCorr, groups_taskDist, \n     groups_taskStep, groups_taskTally, groups_specialization,\n     file = paste0(\"output/SpecializationMetrics/Rdata/\", filename, \"100reps.Rdata\"))\n\n\n",
    "created" : 1509304565392.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3320085982",
    "id" : "F1D75F6",
    "lastKnownWriteTime" : 1509485048,
    "last_content_update" : 1509485048331,
    "path" : "~/Documents/Research/Tarnita Lab/Social Interaction DOL/SocialModel/scripts/1A_ProbThreshModelWithSpecialMetric.R",
    "project_path" : "scripts/1A_ProbThreshModelWithSpecialMetric.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}